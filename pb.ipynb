{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re\n",
    "import nltk\n",
    "import collections\n",
    "import numpy\n",
    "import subprocess\n",
    "import json\n",
    "import multiprocessing\n",
    "import os\n",
    "import gc\n",
    "import sklearn\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "letters_regex = re.compile(r'[а-яА-ЯёЁ]+')\n",
    "def has_letter(s):\n",
    "    return letters_regex.match(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mystem_analyze(text):\n",
    "    p = subprocess.Popen([\"mystem\", \"--format=json\", \"-nigfcsd\"],\n",
    "                         stdin=subprocess.PIPE,\n",
    "                         stdout=subprocess.PIPE,\n",
    "                         stderr=subprocess.PIPE)\n",
    "    stdout, stderr = p.communicate(text.encode('utf-8'))\n",
    "    res_str = stdout.decode('utf-8')\n",
    "    return json.loads('[%s]' % (\",\".join(res_str.splitlines())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_by_sentence_marker(analysis):\n",
    "    last = 0\n",
    "    for i, a in enumerate(analysis):\n",
    "        if a['text'] == '\\\\s':\n",
    "            yield analysis[last:i]\n",
    "            last = i + 1\n",
    "    if analysis[last:]:\n",
    "        yield analysis[last:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TextFeatures = collections.namedtuple('TextFeatures', 'avg_words_in_sentence avg_word_length rel_voc_size rel_hapax_legomena_count rel_pos_count rel_char_count rel_common_word_count')\n",
    "\n",
    "def char_range(start, end):\n",
    "    return [chr(char) for char in range(ord(start), ord(end) + 1)]\n",
    "\n",
    "mystem_pos = ['A', 'ADV', 'ADVPRO', 'ANUM', 'APRO', 'COM', 'CONJ', 'INTJ', 'NUM', 'PART', 'PR', 'S', 'SPRO', 'V']\n",
    "alpha_chars = char_range('а', 'я') + char_range('А', 'Я') + ['ё', 'Ё']\n",
    "num_chars = char_range('0', '9')\n",
    "punct_chars = list(',.?!-:;\"\\'')\n",
    "most_freq_words = ['и', 'в', 'не', 'он', 'на', 'я', 'что', 'тот', 'быть', 'с', 'а', 'весь', 'это', 'как', 'она', 'по', 'но', 'они', 'к', 'у', 'ты', 'из', 'мы', 'за', 'вы', 'так', 'же', 'от', 'сказать', 'этот', 'который', 'мочь', 'человек', 'о', 'один', 'еще', 'бы', 'такой', 'только', 'себя', 'свое', 'какой', 'когда', 'уже', 'для', 'вот', 'кто', 'да', 'говорить', 'год', 'знать', 'мой', 'до', 'или', 'если', 'время', 'рука', 'нет', 'самый', 'ни', 'стать', 'большой', 'даже', 'другой', 'наш', 'свой', 'ну', 'под', 'где', 'дело', 'есть', 'сам', 'раз', 'чтобы', 'два', 'там', 'чем', 'глаз', 'жизнь', 'первый', 'день', 'тута', 'во', 'ничто', 'потом', 'очень', 'со', 'хотеть', 'ли', 'при', 'голова', 'надо', 'без', 'видеть', 'идти', 'теперь', 'тоже', 'стоять', 'друг', 'дом']\n",
    "\n",
    "def get_features(analysis):\n",
    "    words = [w for w in analysis if w.get('analysis')]\n",
    "    text = \"\".join([w['text'] for w in analysis])\n",
    "    sents = list(split_by_sentence_marker(analysis))\n",
    "    words_of_sents = [[w for w in s if w.get('analysis')] for s in sents]\n",
    "    lemma_count = collections.Counter([w['analysis'][0]['lex'] for w in words])\n",
    "    char_count = collections.Counter(text)\n",
    "    \n",
    "    avg_words_in_sentence = numpy.average([len(sent) for sent in words_of_sents])\n",
    "    word_count = len(words)\n",
    "    avg_word_length = numpy.average([len(w['text']) for w in words])\n",
    "    rel_voc_size = len(lemma_count) / word_count\n",
    "    rel_hapax_legomena_count = len([w for w in lemma_count if lemma_count[w] == 1]) / word_count\n",
    "    pos_count = collections.Counter(re.split('[,=()]', w['analysis'][0]['gr'])[0] for w in words)\n",
    "    rel_pos_count = [pos_count[p] / word_count for p in mystem_pos]\n",
    "    rel_char_count = [char_count[c] / len(text) for c in alpha_chars + num_chars + punct_chars]\n",
    "    rel_common_word_count = [lemma_count[w] / word_count for w in most_freq_words]\n",
    "    return TextFeatures(\n",
    "        avg_words_in_sentence=avg_words_in_sentence,\n",
    "        avg_word_length=avg_word_length,\n",
    "        rel_voc_size=rel_voc_size,\n",
    "        rel_hapax_legomena_count=rel_hapax_legomena_count,\n",
    "        rel_pos_count=rel_pos_count,\n",
    "        rel_char_count=rel_char_count,\n",
    "        rel_common_word_count=rel_common_word_count\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vector(f):\n",
    "    return numpy.array([f.avg_words_in_sentence, f.avg_word_length, f.rel_voc_size, f.rel_hapax_legomena_count] +\n",
    "        f.rel_pos_count + f.rel_char_count + f.rel_common_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json\\Abramov_Pryasliny_1_Bratya-i-sestry.edbdyw.158030.fb2.txt.json\n",
      "json\\Abramov_Pryasliny_2_Dve-zimy-i-tri-leta.hU9IrQ.158033.fb2.txt.json\n",
      "json\\Abramov_Pryasliny_3_Puti-pereputya.ktaB0Q.158036.fb2.txt.json\n",
      "json\\Abramov_Pryasliny_4_Dom.FWiheg.158032.fb2.txt.json\n",
      "json\\Aksenov_Zvezdnyy-bilet.HMAp3Q.677.fb2.txt.json\n",
      "json\\Akunin_Priklyucheniya-Erasta-Fandorina_2_Tureckiy-gambit.xK5KVg.305713.fb2.txt.json\n",
      "json\\Akunin_Priklyucheniya-Erasta-Fandorina_7_Statskiy-sovetnik.5irVkw.461230.fb2.txt.json\n",
      "json\\Akunin_Priklyucheniya-Nikolasa-Fandorina_1_Altyn-Tolobas.Uz6tpg.131819.fb2.txt.json\n",
      "json\\Aleksievich_Golosa-Utopii_1_U-voyny-ne-zhenskoe-lico-.VNQLMg.70938.fb2.txt.json\n",
      "json\\Aleksievich_Golosa-Utopii_3_Cinkovye-malchiki.NVa1uw.427115.fb2.txt.json\n",
      "json\\Aleksievich_Golosa-Utopii_5_Vremya-sekond-hend.9FJziw.426019.fb2.txt.json\n",
      "json\\Aleksin_Anatoliy-Aleksin-Sobranie-sochineniy-v-treh-tomah_1_Bezumnaya-Evdokiya.gkQ47w.347944.fb2.txt.json\n",
      "json\\Aleksin_Moy-brat-igraet-na-klarnete.oGHedw.1061.fb2.txt.json\n",
      "json\\Aleksin_Pozdniy-rebenok.pCeYeQ.1068.fb2.txt.json\n",
      "json\\Aleksin_Razdel-imushchestva.qgz_cw.1070.fb2.txt.json\n",
      "json\\Aleksin_Zdorovye-i-bolnye.FJTSAw.122412.fb2.txt.json\n",
      "json\\Aleshkovskiy_Institut-snovideniy.xOQ4GA.141811.fb2.txt.json\n",
      "json\\Aleshkovskiy_Kenguru.wGz24g.1122.fb2.txt.json\n",
      "json\\Aleshkovskiy_Krepost.r3OpIg.435044.fb2.txt.json\n",
      "json\\Aleshkovskiy_Kysh-i-Dvaportfelya_1_Kysh-Dvaportfelya-i-celaya-nedelya.KpGeqQ.452856.fb2.txt.json\n",
      "json\\Andreev_Angelochek.b_uQkA.125257.fb2.txt.json\n",
      "json\\Andreev_Daniil-Andreev-Sobranie-sochineniy-v-4-tomah_2_Zheleznaya-misteriya.r42e2Q.187978.fb2.txt.json\n",
      "json\\Andreev_Iuda-Iskariot.xLnnCQ.288557.fb2.txt.json\n",
      "json\\Andreev_Krasnyy-smeh.X1Y_9Q.65864.fb2.txt.json\n",
      "json\\Andreev_Rasskaz-o-semi-poveshennyh.1L2XlQ.65888.fb2.txt.json\n",
      "json\\Andreev_Roza-Mira.QoAqJg.245158.fb2.txt.json\n",
      "json\\Andreev_Zhizn-Vasiliya-Fiveyskogo.w95XsA.70850.fb2.txt.json\n",
      "json\\Astafev_Astafev-V-P-Sobranie-sochineniy_6_Car-ryba.5R6pfQ.378886.fb2.txt.json\n",
      "json\\Astafev_Pechalnyy-detektiv.ZE6iSA.2714.fb2.txt.json\n",
      "json\\Astafev_Zvezdopad.wFqiSA.132240.fb2.txt.json\n",
      "json\\Averchenko_Podhodcev-i-dvoe-drugih.uidJvg.159231.fb2.txt.json\n",
      "json\\Averchenko_Razumnaya-ekonomiya.AzfKMQ.417321.fb2.txt.json\n",
      "json\\Averchenko_Shutka-mecenata.Tdzcvw.159241.fb2.txt.json\n",
      "json\\Aytmatov_Belyy-parohod.QrQsbA.150859.fb2.txt.json\n",
      "json\\Aytmatov_Pegiy-pes-begushchiy-kraem-morya.ZEyi6g.65797.fb2.txt.json\n",
      "json\\Aytmatov_Plaha.jfGq1Q.65796.fb2.txt.json\n",
      "json\\Bazhov_Malahitovaya-shkatulka-Uralskie-skazy_20_Sinyushkin-kolodec.LjKmdg.373851.fb2.txt.json\n",
      "json\\Bazhov_Mednoy-gory-hozyayka.S2MSYw.143109.fb2.txt.json\n",
      "json\\Belov_Plotnickie-rasskazy.8_wYng.203086.fb2.txt.json\n",
      "json\\Belov_Privychnoe-delo.sZNFWw.158230.fb2.txt.json\n",
      "json\\Belov_Vse-vperedi.lcSc1g.315732.fb2.txt.json\n",
      "json\\Belyaev_Chelovek-poteryavshiy-lico.sSmCLA.145500.fb2.txt.json\n",
      "json\\Bestuzhev-Marlinskiy_Naezdy.x0lp0g.235106.fb2.txt.json\n",
      "json\\Bestuzhev-Marlinskiy_Strashnoe-gadane.bMb4rw.225045.fb2.txt.json\n",
      "json\\Bestuzhev-Marlinskiy_Vecher-na-bivuake.3CreaA.329591.fb2.txt.json\n",
      "json\\Bitov_Prepodavatel-simmetrii.myq3hA.383719.fb2.txt.json\n",
      "json\\Bitov_Pushkinskiy-Dom._PR-6A.246296.fb2.txt.json\n",
      "json\\Bondarev_Bereg.qE7ZrA.472356.fb2.txt.json\n",
      "json\\Bondarev_Goryachiy-sneg.rA-GCg.276869.fb2.txt.json\n",
      "json\\Bondarev_Poslednie-zalpy.bX6XhA.468939.fb2.txt.json\n",
      "json\\Bulgakov_Belaya-gvardiya_1_Belaya-gvardiya.Rgu96w.261423.fb2.txt.json\n",
      "json\\Bulgakov_Master-i-Margarita.7WgsYg.66372.fb2.txt.json\n",
      "json\\Bulgakov_Rokovye-yayca.qhn9-w.66373.fb2.txt.json\n",
      "json\\Bulgakov_Sobache-serdce.EV5sSQ.102112.fb2.txt.json\n",
      "json\\Bulgakov_Zapiski-pokoynika_3_Teatralnyy-roman.3mpwJw.169333.fb2.txt.json\n",
      "json\\Bulychev_Perpendikulyarnyy-mir.g38QIA.408502.fb2.txt.json\n",
      "json\\Bunin_Antonovskie-yabloki.tAKTSA.168440.fb2.txt.json\n",
      "json\\Bunin_Derevnya.SFR3Xw.168499.fb2.txt.json\n",
      "json\\Bunin_Mitina-lyubov.53CEtw.9807.fb2.txt.json\n",
      "json\\Bunin_Suhodol.vMRL8Q.168511.fb2.txt.json\n",
      "json\\Bunin_Zhizn-Arseneva.QDqGYQ.61787.fb2.txt.json\n",
      "json\\Bykov_Dozhit-do-rassveta.iQjlBQ.177179.fb2.txt.json\n",
      "json\\Bykov_Iks.htz4qg.299188.fb2.txt.json\n",
      "json\\Bykov_Mertvym-ne-bolno.H8Y2lQ.164826.fb2.txt.json\n",
      "json\\Bykov_Obelisk.6G1QQw.458130.fb2.txt.json\n",
      "json\\Bykov_Opravdanie.sxqHSA.113370.fb2.txt.json\n",
      "json\\Bykov_Orfografiya.4fQaFQ.113372.fb2.txt.json\n",
      "json\\Bykov_Ostromov-ili-Uchenik-charodeya.YpyxHA.226611.fb2.txt.json\n"
     ]
    }
   ],
   "source": [
    "def get_features_from_file(file):\n",
    "    print(file)\n",
    "    with codecs.open(file, encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        analysis = [json.loads(l) for l in content.splitlines()]\n",
    "        return get_features(analysis)\n",
    "\n",
    "features = [get_features_from_file(os.path.join(\"json\", file)) for file in os.listdir(\"json\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book_files = os.listdir(\"json\")\n",
    "book_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features[0]._asdict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_matrix = numpy.array([get_vector(f) for f in features])\n",
    "scaled_feature_matrix = sklearn.preprocessing.scale(feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpy.shape(scaled_feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eucl_dists = sklearn.metrics.pairwise.pairwise_distances(scaled_feature_matrix)\n",
    "numpy.shape(eucl_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist_graph(dists, names, threshold):\n",
    "    \n",
    "    def author(book):\n",
    "        return book.split('_')[0]\n",
    "    \n",
    "    g = nx.Graph()\n",
    "    g.add_nodes_from((i, dict(author=author(book))) for i, book in enumerate(names))\n",
    "    g.add_weighted_edges_from((i, j, 1/dist) for (i, j), dist in numpy.ndenumerate(dists) if dist <= threshold and dist > 0)\n",
    "    return g\n",
    "\n",
    "nx.write_gml(dist_graph(eucl_dists, book_files, 15), 'graph.gml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_files[120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(numpy.ndenumerate(eucl_dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(enumerate(book_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eucl_dists[100][116]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
